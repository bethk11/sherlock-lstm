{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpq8ekvmQJUBNdBOn3yMRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearpare/sherlock-lstm/blob/main/lstm_project_pt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elisabeth Kam (etk45) \n",
        "\n",
        "I decided to try again using a larger window size of 100 characters as the author of this exercise suggested. Also tried a model with three layers. I had to use a different notebook file because I ran out of the free GPU units, so I switched to my other account to finish this project. "
      ],
      "metadata": {
        "id": "OmdbmV5RMEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.utils.data as data "
      ],
      "metadata": {
        "id": "LFLaeA6PMD-O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQ__9faMant",
        "outputId": "75f93a40-5306-4312-f0dc-5add49783ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3x4PivqLvBe"
      },
      "outputs": [],
      "source": [
        "filename = \"/sherlock.txt\"\n",
        "raw_txt = open(filename, 'r', encoding = 'utf-8').read()\n",
        "sh_raw_txt = raw_txt.lower()\n",
        "sh_raw_txt = raw_txt[:50000]\n",
        "chars = sorted(list(set(raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_txt = raw_txt.lower()\n",
        "raw_txt = raw_txt[:50000]\n",
        "chars = sorted(list(set(raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "JKySTMxYMjYO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_txt)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total characters: \", n_chars)\n",
        "print(\"Total vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFkgnlX_MjhR",
        "outputId": "71a73bae-6bd8-42e3-90d6-d4a5c4ba5e81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  50000\n",
            "Total vocab:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "char_seq_len = 100 #larger window size \n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, n_chars - char_seq_len, 1):\n",
        "    seq_in = raw_txt[i:i + char_seq_len]\n",
        "    seq_out = raw_txt[i + char_seq_len]\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(X_data)\n",
        "print(\"Total patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqyr6M2kMjlS",
        "outputId": "a6d7c25e-20eb-4964-bb8a-03757029ffda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns:  49900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class bookModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "_UOLVqHWMEAn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, char_seq_len, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(y_data)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezVeYrEMEC7",
        "outputId": "97f146e8-3ae3-4d23-8361-7cd2b4ba0f3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49900, 100, 1]) torch.Size([49900])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = bookModel()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char2.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtJ1hPThMEFZ",
        "outputId": "8bb48124-0159-496d-ad11-f5d456d61d0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 146549.047\n",
            "Epoch 1: Cross-entropy: 135440.078\n",
            "Epoch 2: Cross-entropy: 129890.969\n",
            "Epoch 3: Cross-entropy: 126412.086\n",
            "Epoch 4: Cross-entropy: 122646.281\n",
            "Epoch 5: Cross-entropy: 119361.711\n",
            "Epoch 6: Cross-entropy: 116272.125\n",
            "Epoch 7: Cross-entropy: 113397.250\n",
            "Epoch 8: Cross-entropy: 110074.953\n",
            "Epoch 9: Cross-entropy: 106899.484\n",
            "Epoch 10: Cross-entropy: 104489.172\n",
            "Epoch 11: Cross-entropy: 103323.773\n",
            "Epoch 12: Cross-entropy: 100213.281\n",
            "Epoch 13: Cross-entropy: 97640.422\n",
            "Epoch 14: Cross-entropy: 95221.133\n",
            "Epoch 15: Cross-entropy: 93232.602\n",
            "Epoch 16: Cross-entropy: 90617.133\n",
            "Epoch 17: Cross-entropy: 88939.523\n",
            "Epoch 18: Cross-entropy: 86784.281\n",
            "Epoch 19: Cross-entropy: 85144.047\n",
            "Epoch 20: Cross-entropy: 83425.930\n",
            "Epoch 21: Cross-entropy: 82559.914\n",
            "Epoch 22: Cross-entropy: 79065.930\n",
            "Epoch 23: Cross-entropy: 77356.602\n",
            "Epoch 24: Cross-entropy: 75251.961\n",
            "Epoch 25: Cross-entropy: 73369.102\n",
            "Epoch 26: Cross-entropy: 71893.086\n",
            "Epoch 27: Cross-entropy: 70229.859\n",
            "Epoch 28: Cross-entropy: 68890.859\n",
            "Epoch 29: Cross-entropy: 67231.391\n",
            "Epoch 30: Cross-entropy: 65218.930\n",
            "Epoch 31: Cross-entropy: 64082.895\n",
            "Epoch 32: Cross-entropy: 62856.531\n",
            "Epoch 33: Cross-entropy: 61584.500\n",
            "Epoch 34: Cross-entropy: 59575.086\n",
            "Epoch 35: Cross-entropy: 58695.477\n",
            "Epoch 36: Cross-entropy: 57254.992\n",
            "Epoch 37: Cross-entropy: 56050.770\n",
            "Epoch 38: Cross-entropy: 54811.910\n",
            "Epoch 39: Cross-entropy: 53379.105\n",
            "Epoch 40: Cross-entropy: 52302.570\n",
            "Epoch 41: Cross-entropy: 51493.250\n",
            "Epoch 42: Cross-entropy: 50300.371\n",
            "Epoch 43: Cross-entropy: 49176.871\n",
            "Epoch 44: Cross-entropy: 48951.102\n",
            "Epoch 45: Cross-entropy: 47547.844\n",
            "Epoch 46: Cross-entropy: 46609.871\n",
            "Epoch 47: Cross-entropy: 45608.461\n",
            "Epoch 48: Cross-entropy: 44586.520\n",
            "Epoch 49: Cross-entropy: 43553.152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char2.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pnxsDjZMEJQ",
        "outputId": "f9dd1995-b898-40ec-f2ca-2830c76d9f7b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file = \"/sherlock.txt\"\n",
        "raw_txt2 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt2 = raw_txt2.lower()\n",
        "raw_txt2 = raw_txt2[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt2)-seq_len)\n",
        "prompt = raw_txt2[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "zndChQGhMELo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRPx4FRMEPK",
        "outputId": "65387b4a-3999-410f-d2b1-e76dfdc9ee00"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "an.\"\n",
            "\n",
            "\"it is true. and yet--well! i wish she had been of my own\n",
            "station! what a queen she would have\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            " paded uo bo agen to gall to me.\" \n",
            "\"bnd how do you know how he wesy farfenan tery pffisy that i was adfressing and the paiect of his face. \n",
            "\"this is the proeens in the room which i suspected. \n",
            "\n",
            "\"i am to be neutral?\"\n",
            "\n",
            "\"mot at all. the matter was seally and vhat the will do it. you do not inow the soom what is is of tuch as his face, extending down the rtreet. \n",
            "\n",
            "\"mre iade!sr maryy that she has aeen wayle. the photograph is to be an emmsist wotil we drew up un the teene of it. therlock holmes, as i celi the mady, but i have not seen her since in the mort seruln. the land who was the rhout of frowt fact of briony lodge once more from his care and the lefg and a laid to address br ie cotesed. and his been with a quick little landau which had been abandoned a shatinn le the drord toale round to me in my cares. whom have i the honour to address. \n",
            "\n",
            "the ciurch of st. monica, johned and stiftly, but the willg of the soom with a cac, and i mayere that she would have already been made. the doachma\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the model did better with the larger window size. More words are spelled correctly and the sentence meanings are a little easier to parse. "
      ],
      "metadata": {
        "id": "naRqVUN-bFF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class book2Model(nn.Module): #created model with 3 layers \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=3, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "bjpGQmQWS-Kw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = book2Model()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char-3.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HOAey6S-cs",
        "outputId": "6a6c3070-aba4-4d64-910e-42ebd6f38068"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 150608.031\n",
            "Epoch 1: Cross-entropy: 132858.734\n",
            "Epoch 2: Cross-entropy: 126033.406\n",
            "Epoch 3: Cross-entropy: 119851.484\n",
            "Epoch 4: Cross-entropy: 114205.070\n",
            "Epoch 5: Cross-entropy: 109782.203\n",
            "Epoch 6: Cross-entropy: 105523.758\n",
            "Epoch 7: Cross-entropy: 101148.367\n",
            "Epoch 8: Cross-entropy: 98283.305\n",
            "Epoch 9: Cross-entropy: 94372.414\n",
            "Epoch 10: Cross-entropy: 91456.328\n",
            "Epoch 11: Cross-entropy: 89205.617\n",
            "Epoch 12: Cross-entropy: 85368.188\n",
            "Epoch 13: Cross-entropy: 81939.625\n",
            "Epoch 14: Cross-entropy: 79257.695\n",
            "Epoch 15: Cross-entropy: 76299.867\n",
            "Epoch 16: Cross-entropy: 73443.766\n",
            "Epoch 17: Cross-entropy: 72205.406\n",
            "Epoch 18: Cross-entropy: 68441.062\n",
            "Epoch 19: Cross-entropy: 65744.578\n",
            "Epoch 20: Cross-entropy: 63983.066\n",
            "Epoch 21: Cross-entropy: 61112.594\n",
            "Epoch 22: Cross-entropy: 58649.578\n",
            "Epoch 23: Cross-entropy: 56628.828\n",
            "Epoch 24: Cross-entropy: 54666.328\n",
            "Epoch 25: Cross-entropy: 51987.824\n",
            "Epoch 26: Cross-entropy: 50582.551\n",
            "Epoch 27: Cross-entropy: 48712.469\n",
            "Epoch 28: Cross-entropy: 46037.320\n",
            "Epoch 29: Cross-entropy: 45764.047\n",
            "Epoch 30: Cross-entropy: 44329.828\n",
            "Epoch 31: Cross-entropy: 41091.332\n",
            "Epoch 32: Cross-entropy: 39563.602\n",
            "Epoch 33: Cross-entropy: 38238.090\n",
            "Epoch 34: Cross-entropy: 36790.922\n",
            "Epoch 35: Cross-entropy: 35022.297\n",
            "Epoch 36: Cross-entropy: 32819.645\n",
            "Epoch 37: Cross-entropy: 32583.008\n",
            "Epoch 38: Cross-entropy: 30787.113\n",
            "Epoch 39: Cross-entropy: 29350.756\n",
            "Epoch 40: Cross-entropy: 29451.559\n",
            "Epoch 41: Cross-entropy: 29332.363\n",
            "Epoch 42: Cross-entropy: 25742.762\n",
            "Epoch 43: Cross-entropy: 26066.689\n",
            "Epoch 44: Cross-entropy: 24614.412\n",
            "Epoch 45: Cross-entropy: 23464.994\n",
            "Epoch 46: Cross-entropy: 23034.670\n",
            "Epoch 47: Cross-entropy: 21515.846\n",
            "Epoch 48: Cross-entropy: 20846.955\n",
            "Epoch 49: Cross-entropy: 20853.363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char-3.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1gVLieXS-gK",
        "outputId": "aab0912f-4800-4991-ce3e-f8cd7254cba9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file3 = \"/sherlock.txt\"\n",
        "raw_txt3 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt3 = raw_txt3.lower()\n",
        "raw_txt3 = raw_txt3[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt3)-seq_len)\n",
        "prompt = raw_txt3[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "n-mGdxYNS-iF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1gh6-4AS-lI",
        "outputId": "284aa8e3-4381-4188-f776-20ef78111004"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " lucky appearance\n",
            "saved the bridegroom from having to sally out into the streets in\n",
            "search of a best\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            " man. bnd the has the fielt sespous of iis own high-power\n",
            "lenses, would bolngs has to be to mittle metters to me to be ro a creat belicate that i have made myself clear?\"\n",
            "\n",
            "\"i am to be neanly give myst my friend's amazing powers of importance to le to be ro rilence for the pest roint in the conningst of the most singular that the would has sriee and laughed again, in the count von kramm.\"\n",
            "\n",
            "\"then i should have thought a little more. j had not in the past which he had apparently has been myst be an alieied.\"\n",
            "\n",
            "\"to i have not seen, bnd i not be bought under hy wiich i evpected. it was a lews in the oart which has been wayled in the morning. and the world has seen, but as a\n",
            "lover he would have that he will be of the ouher, while a lews then her husband by the ttreet. \n",
            "\n",
            "\"mre iade!and a sueft little prince of a lettle maneau which had been lauely. yhich ie dould not the peculiar construction of the most serulde to the thought, when he stost her own healest and puesteon, but the could not ho fi\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this model the performance was a little better. The sentences are clearer and I can see that where the model got confused with some of the vowels. "
      ],
      "metadata": {
        "id": "u5GZ5ugXbZRl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0gAepYTbvf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}