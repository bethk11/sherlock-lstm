{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHmFUPPPFUsJxn+m981PFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearpare/sherlock-lstm/blob/main/lstm_project_pt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elisabeth Kam (etk45) \n",
        "\n",
        "I decided to try the LSTM model using the author's suggested window size of 100 characters. I also tried a model with 3 layers and window size of 100. Finally, I tried a smaller window size of 50 on both the 2 layer and 3 layer model. Based on the text generated, text clarity and accuracy appeared best with the 3 layer model. The difference in window size did not seem to alter the text a lot. I did note, though, that the model with the smaller window size trained much faster. Also, for some reason the window size = 50 and 2 layer model experiment needed to try generate a text prompt 2 or 3 times before legible text was generated with the prompt. "
      ],
      "metadata": {
        "id": "OmdbmV5RMEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.utils.data as data "
      ],
      "metadata": {
        "id": "LFLaeA6PMD-O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQ__9faMant",
        "outputId": "75f93a40-5306-4312-f0dc-5add49783ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with window size = 100 starts here. "
      ],
      "metadata": {
        "id": "J0DoGnPevX_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "N3x4PivqLvBe"
      },
      "outputs": [],
      "source": [
        "filename = \"/sherlock.txt\"\n",
        "sh_raw_txt = open(filename, 'r', encoding = 'utf-8').read()\n",
        "sh_raw_txt = sh_raw_txt.lower()\n",
        "sh_raw_txt = sh_raw_txt[:50000]\n",
        "chars = sorted(list(set(sh_raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(sh_raw_txt)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total characters: \", n_chars)\n",
        "print(\"Total vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFkgnlX_MjhR",
        "outputId": "0d605345-0b89-4a0b-c08e-7835e6b96284"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  50000\n",
            "Total vocab:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "char_seq_len = 100 #larger window size \n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, n_chars - char_seq_len, 1):\n",
        "    seq_in = sh_raw_txt[i:i + char_seq_len]\n",
        "    seq_out = sh_raw_txt[i + char_seq_len]\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(X_data)\n",
        "print(\"Total patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqyr6M2kMjlS",
        "outputId": "d995ebe6-f04a-43bf-ffd3-427504aee916"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns:  49900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with 2 layers is here. "
      ],
      "metadata": {
        "id": "z9Q68F2LCKmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class bookModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "_UOLVqHWMEAn"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, char_seq_len, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(y_data)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezVeYrEMEC7",
        "outputId": "0fdde27d-3f06-41d9-f5d5-47b6483530ab"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49900, 100, 1]) torch.Size([49900])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = bookModel()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char2.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtJ1hPThMEFZ",
        "outputId": "31db3903-0102-41a2-dd29-9aaecb56d467"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 146781.094\n",
            "Epoch 1: Cross-entropy: 135529.297\n",
            "Epoch 2: Cross-entropy: 130175.898\n",
            "Epoch 3: Cross-entropy: 126038.141\n",
            "Epoch 4: Cross-entropy: 122130.812\n",
            "Epoch 5: Cross-entropy: 119098.273\n",
            "Epoch 6: Cross-entropy: 116268.281\n",
            "Epoch 7: Cross-entropy: 112764.578\n",
            "Epoch 8: Cross-entropy: 109724.141\n",
            "Epoch 9: Cross-entropy: 107512.188\n",
            "Epoch 10: Cross-entropy: 104617.172\n",
            "Epoch 11: Cross-entropy: 102173.609\n",
            "Epoch 12: Cross-entropy: 100138.430\n",
            "Epoch 13: Cross-entropy: 99255.789\n",
            "Epoch 14: Cross-entropy: 95459.695\n",
            "Epoch 15: Cross-entropy: 93677.250\n",
            "Epoch 16: Cross-entropy: 91449.516\n",
            "Epoch 17: Cross-entropy: 89326.305\n",
            "Epoch 18: Cross-entropy: 87645.375\n",
            "Epoch 19: Cross-entropy: 86385.469\n",
            "Epoch 20: Cross-entropy: 84793.602\n",
            "Epoch 21: Cross-entropy: 82231.797\n",
            "Epoch 22: Cross-entropy: 80089.805\n",
            "Epoch 23: Cross-entropy: 77776.555\n",
            "Epoch 24: Cross-entropy: 76789.234\n",
            "Epoch 25: Cross-entropy: 74719.570\n",
            "Epoch 26: Cross-entropy: 73619.094\n",
            "Epoch 27: Cross-entropy: 70780.656\n",
            "Epoch 28: Cross-entropy: 69807.688\n",
            "Epoch 29: Cross-entropy: 68576.477\n",
            "Epoch 30: Cross-entropy: 69469.961\n",
            "Epoch 31: Cross-entropy: 66021.594\n",
            "Epoch 32: Cross-entropy: 63968.570\n",
            "Epoch 33: Cross-entropy: 63044.680\n",
            "Epoch 34: Cross-entropy: 61762.859\n",
            "Epoch 35: Cross-entropy: 60318.664\n",
            "Epoch 36: Cross-entropy: 58278.805\n",
            "Epoch 37: Cross-entropy: 59696.473\n",
            "Epoch 38: Cross-entropy: 56609.691\n",
            "Epoch 39: Cross-entropy: 55273.754\n",
            "Epoch 40: Cross-entropy: 54313.355\n",
            "Epoch 41: Cross-entropy: 52887.668\n",
            "Epoch 42: Cross-entropy: 52120.223\n",
            "Epoch 43: Cross-entropy: 51270.676\n",
            "Epoch 44: Cross-entropy: 50225.781\n",
            "Epoch 45: Cross-entropy: 49761.016\n",
            "Epoch 46: Cross-entropy: 49231.484\n",
            "Epoch 47: Cross-entropy: 48011.883\n",
            "Epoch 48: Cross-entropy: 46564.668\n",
            "Epoch 49: Cross-entropy: 46121.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char2.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pnxsDjZMEJQ",
        "outputId": "a4fa83d5-e290-4aa9-a402-b1ead072c211"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file = \"/sherlock.txt\"\n",
        "raw_txt2 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt2 = raw_txt2.lower()\n",
        "raw_txt2 = raw_txt2[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt2)-seq_len)\n",
        "prompt = raw_txt2[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "zndChQGhMELo"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRPx4FRMEPK",
        "outputId": "7c0395d9-77a0-4828-a0d9-e4308687d743"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "st go to life itself, which is always far more\n",
            "daring than any effort of the imagination.\"\n",
            "\n",
            "\"a propo\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            "witeon which i should nere that i have aeen oo doubt that the was there in a dener alear in a ger casi. and i have not seens her heai. \n",
            "\"then, what dod howerest to thmek in in to make a mystery.\" \n",
            "\"i am suree that so the coon which i have aeen watch form the street. \n",
            "\"as the cound to bone in the mant sery wour hes sodeess to and down in the mady, but she sase the door of the sor dound have been carsed me aoong the mat who werk foom the house of aviony ootilngs, and i was a fouble so ae on the sodee of av the house. and the cooner of the street. \n",
            "\n",
            "\"then ther?\"\n",
            "\n",
            "\"i was allaidd toundrs.\" \n",
            "\"i was allara\"\n",
            "\n",
            "\"to an and that in the len's puine mer. and the coack of his singular. \n",
            "\"as ie spok there is a lans so be an active me and lnto besirting then the photograph and amleared so the shouger. \"i said the lang of the sor dound have been harde to she hnuse of ariony lodge of aidins to the coor of his hands woon the shouleraphon with the was of the sort without a sityonaphon wou armseit io the so\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the text generated is okay. I'm able to identify some sentences and words even when misspelled. "
      ],
      "metadata": {
        "id": "naRqVUN-bFF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with 3 layers is here. "
      ],
      "metadata": {
        "id": "BwtKdJUyCdWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class book2Model(nn.Module): #created model with 3 layers \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=3, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "bjpGQmQWS-Kw"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with 3 layer model. "
      ],
      "metadata": {
        "id": "UISGcuAGChr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = book2Model()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char-3.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HOAey6S-cs",
        "outputId": "45c5efcb-5db1-4b51-b0d9-10a6737bb33c"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 150704.516\n",
            "Epoch 1: Cross-entropy: 132908.656\n",
            "Epoch 2: Cross-entropy: 125894.719\n",
            "Epoch 3: Cross-entropy: 120314.141\n",
            "Epoch 4: Cross-entropy: 115529.609\n",
            "Epoch 5: Cross-entropy: 109486.078\n",
            "Epoch 6: Cross-entropy: 105817.461\n",
            "Epoch 7: Cross-entropy: 104835.742\n",
            "Epoch 8: Cross-entropy: 97887.938\n",
            "Epoch 9: Cross-entropy: 94645.336\n",
            "Epoch 10: Cross-entropy: 91818.727\n",
            "Epoch 11: Cross-entropy: 88327.641\n",
            "Epoch 12: Cross-entropy: 85569.727\n",
            "Epoch 13: Cross-entropy: 82476.875\n",
            "Epoch 14: Cross-entropy: 79475.359\n",
            "Epoch 15: Cross-entropy: 77831.297\n",
            "Epoch 16: Cross-entropy: 74107.625\n",
            "Epoch 17: Cross-entropy: 72303.164\n",
            "Epoch 18: Cross-entropy: 69078.734\n",
            "Epoch 19: Cross-entropy: 67962.422\n",
            "Epoch 20: Cross-entropy: 64573.617\n",
            "Epoch 21: Cross-entropy: 61997.953\n",
            "Epoch 22: Cross-entropy: 60507.984\n",
            "Epoch 23: Cross-entropy: 57628.055\n",
            "Epoch 24: Cross-entropy: 55925.523\n",
            "Epoch 25: Cross-entropy: 53027.613\n",
            "Epoch 26: Cross-entropy: 51084.098\n",
            "Epoch 27: Cross-entropy: 49110.070\n",
            "Epoch 28: Cross-entropy: 47487.148\n",
            "Epoch 29: Cross-entropy: 45358.652\n",
            "Epoch 30: Cross-entropy: 44984.250\n",
            "Epoch 31: Cross-entropy: 41986.801\n",
            "Epoch 32: Cross-entropy: 39816.793\n",
            "Epoch 33: Cross-entropy: 39517.344\n",
            "Epoch 34: Cross-entropy: 38266.992\n",
            "Epoch 35: Cross-entropy: 36040.168\n",
            "Epoch 36: Cross-entropy: 35245.176\n",
            "Epoch 37: Cross-entropy: 32997.438\n",
            "Epoch 38: Cross-entropy: 31638.891\n",
            "Epoch 39: Cross-entropy: 32177.053\n",
            "Epoch 40: Cross-entropy: 30122.184\n",
            "Epoch 41: Cross-entropy: 27760.756\n",
            "Epoch 42: Cross-entropy: 27027.178\n",
            "Epoch 43: Cross-entropy: 26330.758\n",
            "Epoch 44: Cross-entropy: 24810.406\n",
            "Epoch 45: Cross-entropy: 23901.832\n",
            "Epoch 46: Cross-entropy: 23066.986\n",
            "Epoch 47: Cross-entropy: 22231.986\n",
            "Epoch 48: Cross-entropy: 21466.160\n",
            "Epoch 49: Cross-entropy: 20950.779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char-3.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1gVLieXS-gK",
        "outputId": "694d9d60-9b0f-4c17-9492-95e20ff3aff5"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file3 = \"/sherlock.txt\"\n",
        "raw_txt3 = open(file3, 'r', encoding = 'utf-8').read()\n",
        "raw_txt3 = raw_txt3.lower()\n",
        "raw_txt3 = raw_txt3[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt3)-seq_len)\n",
        "prompt = raw_txt3[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "n-mGdxYNS-iF"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1gh6-4AS-lI",
        "outputId": "29c673c5-97fe-4c54-b268-fd677ee316d2"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "aisle like any other idler who has dropped into a church.\n",
            "suddenly, to my surprise, the three at th\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            "e tittant fallly and sushed to the chamber which\n",
            "had been ly rllplight in the character of an amiable and laughed to the cegebrated. the coiver look of his own high-power\n",
            "with a fear half any other purpise and colpanion, but the steps of miss at the signal i tosk the soom and alose so she ceattifam which\n",
            "had been lys upon the steps; she watched us with the street. and i have not seen her since. i rose, and i mean\n",
            "to see holmes as he lay\n",
            "upon the smaller crimes, and in a gentleman wpon his chair and paserved in men. and the man who wrote the soom and cound seven fuodred of street, and the lady of the steps of my mwst\n",
            "be on the sight side, well furnished, with a bhepce puttosseo, h ment the staged oe steeertande\n",
            "ar the coor of briony lodge, as it munt ae aought under half a crown a packet. it is alwo in the least interested. but\n",
            "the could not lerely what she has seen whether he was seized with compunction of the sentoeraph which had been lys ceartifrl which had been lys upon the steps; s\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think for this model the performance was a little better. The sentences resemble the original text more closely (at least to my knowledge of Sherlock Holmes). "
      ],
      "metadata": {
        "id": "u5GZ5ugXbZRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with window size = 50 starts here. "
      ],
      "metadata": {
        "id": "I0udlmE5v7SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename50 = \"/sherlock.txt\"\n",
        "raw_txt50 = open(filename50, 'r', encoding = 'utf-8').read()\n",
        "raw_txt50 = raw_txt50.lower()\n",
        "raw_txt50 = raw_txt50[:50000]\n",
        "chars = sorted(list(set(raw_txt50)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "lpGLvK2Iv_Pi"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_txt50)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total characters: \", n_chars)\n",
        "print(\"Total vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV3dIxhOwIA8",
        "outputId": "8b2fe2bf-6df5-4d9e-92aa-c555f2c60d17"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  50000\n",
            "Total vocab:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_seq_len = 50\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, n_chars - char_seq_len, 1):\n",
        "    seq_in = raw_txt50[i:i + char_seq_len]\n",
        "    seq_out = raw_txt50[i + char_seq_len]\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(X_data)\n",
        "print(\"Total patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkqAfNcvv9_E",
        "outputId": "7c5814c1-66f9-4802-9213-d29184d962dd"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns:  49950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, char_seq_len, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(y_data)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3o50_UXwb-5",
        "outputId": "46d59094-f00a-4d36-c9ea-b90801bb60ad"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49950, 50, 1]) torch.Size([49950])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model50 = bookModel()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model50.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model50.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model50.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model50(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model50.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model50(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model50.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char-50.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DriAn6a8wh15",
        "outputId": "f9d093f3-f3e8-48fe-d2b7-26661d734c30"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 145674.844\n",
            "Epoch 1: Cross-entropy: 134426.000\n",
            "Epoch 2: Cross-entropy: 129494.883\n",
            "Epoch 3: Cross-entropy: 125980.109\n",
            "Epoch 4: Cross-entropy: 121899.492\n",
            "Epoch 5: Cross-entropy: 118436.031\n",
            "Epoch 6: Cross-entropy: 115019.148\n",
            "Epoch 7: Cross-entropy: 112203.992\n",
            "Epoch 8: Cross-entropy: 109002.766\n",
            "Epoch 9: Cross-entropy: 106233.922\n",
            "Epoch 10: Cross-entropy: 104881.367\n",
            "Epoch 11: Cross-entropy: 101396.672\n",
            "Epoch 12: Cross-entropy: 99150.281\n",
            "Epoch 13: Cross-entropy: 96492.945\n",
            "Epoch 14: Cross-entropy: 94176.617\n",
            "Epoch 15: Cross-entropy: 92015.469\n",
            "Epoch 16: Cross-entropy: 89741.727\n",
            "Epoch 17: Cross-entropy: 88383.641\n",
            "Epoch 18: Cross-entropy: 85589.969\n",
            "Epoch 19: Cross-entropy: 84468.227\n",
            "Epoch 20: Cross-entropy: 82199.102\n",
            "Epoch 21: Cross-entropy: 81272.930\n",
            "Epoch 22: Cross-entropy: 78466.141\n",
            "Epoch 23: Cross-entropy: 76956.281\n",
            "Epoch 24: Cross-entropy: 74749.031\n",
            "Epoch 25: Cross-entropy: 72820.562\n",
            "Epoch 26: Cross-entropy: 70892.922\n",
            "Epoch 27: Cross-entropy: 69317.289\n",
            "Epoch 28: Cross-entropy: 68648.812\n",
            "Epoch 29: Cross-entropy: 66651.477\n",
            "Epoch 30: Cross-entropy: 64599.344\n",
            "Epoch 31: Cross-entropy: 63465.246\n",
            "Epoch 32: Cross-entropy: 61972.391\n",
            "Epoch 33: Cross-entropy: 61395.152\n",
            "Epoch 34: Cross-entropy: 59854.793\n",
            "Epoch 35: Cross-entropy: 57929.496\n",
            "Epoch 36: Cross-entropy: 56246.246\n",
            "Epoch 37: Cross-entropy: 54712.086\n",
            "Epoch 38: Cross-entropy: 56084.941\n",
            "Epoch 39: Cross-entropy: 52627.758\n",
            "Epoch 40: Cross-entropy: 51386.586\n",
            "Epoch 41: Cross-entropy: 50412.988\n",
            "Epoch 42: Cross-entropy: 49724.949\n",
            "Epoch 43: Cross-entropy: 48209.770\n",
            "Epoch 44: Cross-entropy: 48039.039\n",
            "Epoch 45: Cross-entropy: 46118.402\n",
            "Epoch 46: Cross-entropy: 46012.727\n",
            "Epoch 47: Cross-entropy: 46272.586\n",
            "Epoch 48: Cross-entropy: 43941.457\n",
            "Epoch 49: Cross-entropy: 42354.918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char-50.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model50.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NhUCF04wh6f",
        "outputId": "d117aec4-22ec-4259-d9a0-0627551d84e7"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file50 = \"/sherlock.txt\"\n",
        "raw_2txt = open(file50, 'r', encoding = 'utf-8').read()\n",
        "raw_2txt = raw_2txt.lower()\n",
        "raw_2txt = raw_2txt[:50000]\n",
        "seq_len = 50\n",
        "start = np.random.randint(0, len(raw_2txt)-seq_len)\n",
        "prompt = raw_2txt[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "A4WWwlHqwyuB"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model50.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Ij4J1jw1iY",
        "outputId": "79eef3c9-090e-47e9-fe8f-bc13772bbc12"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "struck savagely at each other with their fists and\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            " sticks. holmes whistled.\n",
            "\n",
            "\"a pair, by the sousd it ore of the lont lerters what he was a froup of betpared with a fear half a coont of bettered in the part he was pllitive from his chair and paced up and down the room which he had apparently adjusted the man who wrote the toom and cootedled. the dourse of the street. \n",
            "it is a\n",
            "customary contraction like out of the lont lenter. it was ne it a slall sllenni pf herting in the conner of the street. \n",
            "\"indied! my must be recovered.\"\n",
            "\n",
            "\"we have tried and a galf oo the steps; she wasched us and down in front of the forrer. i saw his motere men\n",
            "and the lady; but just as he reached\n",
            "her hands up his chair and paced up and down the room which he had apparently adjusted the man who wrote the toom and cootedled. the dourse of the street. \n",
            "it is a\n",
            "customary contraction like out of the lont lenter. it was ne it a slall sllenni pf herting in the conner of the street. \n",
            "\"indied! my must be recovered.\"\n",
            "\n",
            "\"we have tried and a galf oo the steps; she wasched u\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text looks decent, not significantly better or worse than the previous experiment with window size = 100. "
      ],
      "metadata": {
        "id": "IEe4naaZC2dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with 3 layer model and window size = 50 is here. "
      ],
      "metadata": {
        "id": "4V6pQp7v9zIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model50 = book2Model()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model50.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model50.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model50.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model50(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model50.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model50(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model50.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char-50-2.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDr4Joh09yIL",
        "outputId": "87c2139e-37b0-4aac-ad1f-65816453a9de"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 150352.578\n",
            "Epoch 1: Cross-entropy: 132503.484\n",
            "Epoch 2: Cross-entropy: 125117.719\n",
            "Epoch 3: Cross-entropy: 118761.719\n",
            "Epoch 4: Cross-entropy: 116234.234\n",
            "Epoch 5: Cross-entropy: 108954.469\n",
            "Epoch 6: Cross-entropy: 109986.922\n",
            "Epoch 7: Cross-entropy: 103127.672\n",
            "Epoch 8: Cross-entropy: 99225.438\n",
            "Epoch 9: Cross-entropy: 94391.773\n",
            "Epoch 10: Cross-entropy: 91105.312\n",
            "Epoch 11: Cross-entropy: 89058.500\n",
            "Epoch 12: Cross-entropy: 85809.352\n",
            "Epoch 13: Cross-entropy: 83689.523\n",
            "Epoch 14: Cross-entropy: 79318.953\n",
            "Epoch 15: Cross-entropy: 76596.773\n",
            "Epoch 16: Cross-entropy: 74223.164\n",
            "Epoch 17: Cross-entropy: 72065.070\n",
            "Epoch 18: Cross-entropy: 70673.703\n",
            "Epoch 19: Cross-entropy: 66093.320\n",
            "Epoch 20: Cross-entropy: 70347.875\n",
            "Epoch 21: Cross-entropy: 62649.438\n",
            "Epoch 22: Cross-entropy: 60786.703\n",
            "Epoch 23: Cross-entropy: 58023.207\n",
            "Epoch 24: Cross-entropy: 56147.645\n",
            "Epoch 25: Cross-entropy: 54004.727\n",
            "Epoch 26: Cross-entropy: 54471.883\n",
            "Epoch 27: Cross-entropy: 49944.621\n",
            "Epoch 28: Cross-entropy: 48277.414\n",
            "Epoch 29: Cross-entropy: 46087.855\n",
            "Epoch 30: Cross-entropy: 44307.168\n",
            "Epoch 31: Cross-entropy: 42958.887\n",
            "Epoch 32: Cross-entropy: 41652.332\n",
            "Epoch 33: Cross-entropy: 40932.008\n",
            "Epoch 34: Cross-entropy: 37976.980\n",
            "Epoch 35: Cross-entropy: 37184.781\n",
            "Epoch 36: Cross-entropy: 38967.664\n",
            "Epoch 37: Cross-entropy: 34759.551\n",
            "Epoch 38: Cross-entropy: 32149.174\n",
            "Epoch 39: Cross-entropy: 32548.189\n",
            "Epoch 40: Cross-entropy: 30284.643\n",
            "Epoch 41: Cross-entropy: 30667.068\n",
            "Epoch 42: Cross-entropy: 28633.410\n",
            "Epoch 43: Cross-entropy: 27622.594\n",
            "Epoch 44: Cross-entropy: 26123.875\n",
            "Epoch 45: Cross-entropy: 33689.898\n",
            "Epoch 46: Cross-entropy: 26143.348\n",
            "Epoch 47: Cross-entropy: 23513.260\n",
            "Epoch 48: Cross-entropy: 23206.164\n",
            "Epoch 49: Cross-entropy: 21579.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char-50-2.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model50.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kopKnB4C9yWs",
        "outputId": "6213798d-43ee-4cd5-e7aa-4767aeaae623"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file50_2 = \"/sherlock.txt\"\n",
        "raw_2txt_2 = open(file50_2, 'r', encoding = 'utf-8').read()\n",
        "raw_2txt_2 = raw_2txt_2.lower()\n",
        "raw_2txt_2 = raw_2txt_2[:50000]\n",
        "seq_len = 50\n",
        "start = np.random.randint(0, len(raw_2txt_2)-seq_len)\n",
        "prompt = raw_2txt_2[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "SSAys9i_9yeJ"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model50.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lEmrgAU9yj9",
        "outputId": "fb30f0a4-fb45-43bb-d2cb-42a74766901d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "e speaks of irene adler, or when he refers to her\n",
            "\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            "cound be me that the would be the matt post.\"\n",
            "\n",
            "\"nh, then we have the pro house.\"\n",
            "\n",
            "\"i was aware of that?\" asked the king had doue in a dreat scandal through the street. \"it is a\n",
            "customary contraction like out of the lont lenter. it was ne it a slall sllenni pf herting in the conner of the street. \n",
            "\"indied! my must be recovered.\"\n",
            "\n",
            "\"we have tried and a galf oo the steps; she wasched us and down in front of the forrer. i saw his motere men\n",
            "and the lady; but just as he reached\n",
            "her hands up his chair and paced up and down the room which he had apparently adjusted the man who wrote the toom and cootedled. the dourse of the street. \n",
            "it is a\n",
            "customary contraction like out of the lont lenter. it was ne it a slall sllenni pf herting in the conner of the street. \n",
            "\"indied! my must be recovered.\"\n",
            "\n",
            "\"we have tried and a galf oo the steps; she wasched us and down in front of the forrer. i saw his motere men\n",
            "and the lady; but just as he reached\n",
            "her hands up his chair and paced up and down the room which\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, text generation is better with 3 layer model. The sentences here are a lot clearer and seem closer to the original text. "
      ],
      "metadata": {
        "id": "tid2N42fDLG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last note: the cross entropy decreased more for the 3 layer model than the 2 layer model regardless of window size. For the 2 layer model, the cross entropy was relatively similar for both window sizes. Based on the cross entropy values, the 3 layer model overall has the better performance. "
      ],
      "metadata": {
        "id": "LXFj0EOqIrnq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqhtIZcDJViu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}