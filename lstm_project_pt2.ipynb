{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNihSlRwdsJOnS2ia/SIckG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearpare/sherlock-lstm/blob/main/lstm_project_pt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elisabeth Kam (etk45) \n",
        "\n",
        "I decided to try again using a larger window size of 100 characters as the author of this exercise suggested. Also tried a model with three layers. I had to use a different notebook file because I ran out of the free GPU units, so I switched to my other account to finish this project. "
      ],
      "metadata": {
        "id": "OmdbmV5RMEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.utils.data as data "
      ],
      "metadata": {
        "id": "LFLaeA6PMD-O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okQ__9faMant",
        "outputId": "75f93a40-5306-4312-f0dc-5add49783ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "N3x4PivqLvBe"
      },
      "outputs": [],
      "source": [
        "filename = \"/sherlock.txt\"\n",
        "raw_txt = open(filename, 'r', encoding = 'utf-8').read()\n",
        "sh_raw_txt = sh_raw_txt.lower()\n",
        "sh_raw_txt = sh_raw_txt[:50000]\n",
        "chars = sorted(list(set(raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sh_raw_txt = sh_raw_txt.lower()\n",
        "sh_raw_txt = sh_raw_txt[:50000]\n",
        "chars = sorted(list(set(sh_raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "JKySTMxYMjYO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(sh_raw_txt)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total characters: \", n_chars)\n",
        "print(\"Total vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFkgnlX_MjhR",
        "outputId": "408a0f7f-b99d-4ca9-be41-b984b519753b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  50000\n",
            "Total vocab:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "char_seq_len = 100 #larger window size \n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, n_chars - char_seq_len, 1):\n",
        "    seq_in = sh_raw_txt[i:i + char_seq_len]\n",
        "    seq_out = sh_raw_txt[i + char_seq_len]\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(X_data)\n",
        "print(\"Total patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqyr6M2kMjlS",
        "outputId": "02ef3a7c-d772-4d59-9d70-67eea6c9f7ab"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns:  49900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class bookModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "_UOLVqHWMEAn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, char_seq_len, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(y_data)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezVeYrEMEC7",
        "outputId": "bf43efe8-67bc-476b-d312-1ce6c1ecd75a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49900, 100, 1]) torch.Size([49900])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = bookModel()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char2.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtJ1hPThMEFZ",
        "outputId": "71e635c5-5158-4722-aa37-f1e9b8fd79c3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 146423.734\n",
            "Epoch 1: Cross-entropy: 134507.641\n",
            "Epoch 2: Cross-entropy: 129851.812\n",
            "Epoch 3: Cross-entropy: 126100.938\n",
            "Epoch 4: Cross-entropy: 122343.914\n",
            "Epoch 5: Cross-entropy: 119056.297\n",
            "Epoch 6: Cross-entropy: 115243.305\n",
            "Epoch 7: Cross-entropy: 112570.562\n",
            "Epoch 8: Cross-entropy: 109675.312\n",
            "Epoch 9: Cross-entropy: 107265.148\n",
            "Epoch 10: Cross-entropy: 104756.805\n",
            "Epoch 11: Cross-entropy: 102448.195\n",
            "Epoch 12: Cross-entropy: 99789.516\n",
            "Epoch 13: Cross-entropy: 97889.625\n",
            "Epoch 14: Cross-entropy: 96005.938\n",
            "Epoch 15: Cross-entropy: 93885.320\n",
            "Epoch 16: Cross-entropy: 91254.430\n",
            "Epoch 17: Cross-entropy: 89040.836\n",
            "Epoch 18: Cross-entropy: 87437.484\n",
            "Epoch 19: Cross-entropy: 85348.969\n",
            "Epoch 20: Cross-entropy: 83932.414\n",
            "Epoch 21: Cross-entropy: 81658.484\n",
            "Epoch 22: Cross-entropy: 79848.930\n",
            "Epoch 23: Cross-entropy: 77898.703\n",
            "Epoch 24: Cross-entropy: 76688.164\n",
            "Epoch 25: Cross-entropy: 76028.609\n",
            "Epoch 26: Cross-entropy: 73137.641\n",
            "Epoch 27: Cross-entropy: 73364.773\n",
            "Epoch 28: Cross-entropy: 70236.875\n",
            "Epoch 29: Cross-entropy: 68403.633\n",
            "Epoch 30: Cross-entropy: 68073.008\n",
            "Epoch 31: Cross-entropy: 65374.859\n",
            "Epoch 32: Cross-entropy: 64527.574\n",
            "Epoch 33: Cross-entropy: 62910.605\n",
            "Epoch 34: Cross-entropy: 61491.332\n",
            "Epoch 35: Cross-entropy: 60357.574\n",
            "Epoch 36: Cross-entropy: 59123.562\n",
            "Epoch 37: Cross-entropy: 57654.191\n",
            "Epoch 38: Cross-entropy: 56625.488\n",
            "Epoch 39: Cross-entropy: 54843.219\n",
            "Epoch 40: Cross-entropy: 54899.832\n",
            "Epoch 41: Cross-entropy: 53059.184\n",
            "Epoch 42: Cross-entropy: 51906.008\n",
            "Epoch 43: Cross-entropy: 51692.215\n",
            "Epoch 44: Cross-entropy: 51438.840\n",
            "Epoch 45: Cross-entropy: 50000.223\n",
            "Epoch 46: Cross-entropy: 49594.121\n",
            "Epoch 47: Cross-entropy: 47551.867\n",
            "Epoch 48: Cross-entropy: 46826.805\n",
            "Epoch 49: Cross-entropy: 46207.477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char2.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pnxsDjZMEJQ",
        "outputId": "e68eb047-db55-4986-f99f-2853913570d4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file = \"/sherlock.txt\"\n",
        "raw_txt2 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt2 = raw_txt2.lower()\n",
        "raw_txt2 = raw_txt2[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt2)-seq_len)\n",
        "prompt = raw_txt2[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "zndChQGhMELo"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRPx4FRMEPK",
        "outputId": "e083a16a-4ed6-4eb2-9913-0ec7a312d59b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "over-precipitance may ruin all.\"\n",
            "\n",
            "\"and now?\" i asked.\n",
            "\n",
            "\"our quest is practically finished. i shall c\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            "e bole in the matter which he had appalled the most in front of the seiple that i have have the lady of to may her hand, and i have not heard him in the soreet, and i have not seen be oe the lady of his cabres and have a coy of fire, and a gond coon think i have been toinge for the coorer of the street. \n",
            "\"the court so seruon.\"\n",
            "\n",
            "\"then they wank to you.\"\n",
            "\n",
            "\"but you have the han shere i was a lareeine she hing of the seryer. i had been told the lady of his cabres and have a coy of fire, and a gond coon think i have been tooe that i was a wars stice in a sery sitel of his oonmcers. but it is aloosmanion foo the cherr and leuter iis fand and at the house and suiftlar thise oaster of it as once to see me the lady, aut it is and lades of the sort seisle she shoule brongh mot at once to the lett this would be anl arirneting her fard, and i have not seen be an ont a sary site of the lady of his cabe,  ny crienee mf to sear the seoule. it was a lew which i have have the lady of to may her hands u\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think the model did better with the larger window size. More words are spelled correctly and the sentence meanings are a little easier to parse. "
      ],
      "metadata": {
        "id": "naRqVUN-bFF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class book2Model(nn.Module): #created model with 3 layers \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=3, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "bjpGQmQWS-Kw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = book2Model()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char-3.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HOAey6S-cs",
        "outputId": "6a6c3070-aba4-4d64-910e-42ebd6f38068"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 150608.031\n",
            "Epoch 1: Cross-entropy: 132858.734\n",
            "Epoch 2: Cross-entropy: 126033.406\n",
            "Epoch 3: Cross-entropy: 119851.484\n",
            "Epoch 4: Cross-entropy: 114205.070\n",
            "Epoch 5: Cross-entropy: 109782.203\n",
            "Epoch 6: Cross-entropy: 105523.758\n",
            "Epoch 7: Cross-entropy: 101148.367\n",
            "Epoch 8: Cross-entropy: 98283.305\n",
            "Epoch 9: Cross-entropy: 94372.414\n",
            "Epoch 10: Cross-entropy: 91456.328\n",
            "Epoch 11: Cross-entropy: 89205.617\n",
            "Epoch 12: Cross-entropy: 85368.188\n",
            "Epoch 13: Cross-entropy: 81939.625\n",
            "Epoch 14: Cross-entropy: 79257.695\n",
            "Epoch 15: Cross-entropy: 76299.867\n",
            "Epoch 16: Cross-entropy: 73443.766\n",
            "Epoch 17: Cross-entropy: 72205.406\n",
            "Epoch 18: Cross-entropy: 68441.062\n",
            "Epoch 19: Cross-entropy: 65744.578\n",
            "Epoch 20: Cross-entropy: 63983.066\n",
            "Epoch 21: Cross-entropy: 61112.594\n",
            "Epoch 22: Cross-entropy: 58649.578\n",
            "Epoch 23: Cross-entropy: 56628.828\n",
            "Epoch 24: Cross-entropy: 54666.328\n",
            "Epoch 25: Cross-entropy: 51987.824\n",
            "Epoch 26: Cross-entropy: 50582.551\n",
            "Epoch 27: Cross-entropy: 48712.469\n",
            "Epoch 28: Cross-entropy: 46037.320\n",
            "Epoch 29: Cross-entropy: 45764.047\n",
            "Epoch 30: Cross-entropy: 44329.828\n",
            "Epoch 31: Cross-entropy: 41091.332\n",
            "Epoch 32: Cross-entropy: 39563.602\n",
            "Epoch 33: Cross-entropy: 38238.090\n",
            "Epoch 34: Cross-entropy: 36790.922\n",
            "Epoch 35: Cross-entropy: 35022.297\n",
            "Epoch 36: Cross-entropy: 32819.645\n",
            "Epoch 37: Cross-entropy: 32583.008\n",
            "Epoch 38: Cross-entropy: 30787.113\n",
            "Epoch 39: Cross-entropy: 29350.756\n",
            "Epoch 40: Cross-entropy: 29451.559\n",
            "Epoch 41: Cross-entropy: 29332.363\n",
            "Epoch 42: Cross-entropy: 25742.762\n",
            "Epoch 43: Cross-entropy: 26066.689\n",
            "Epoch 44: Cross-entropy: 24614.412\n",
            "Epoch 45: Cross-entropy: 23464.994\n",
            "Epoch 46: Cross-entropy: 23034.670\n",
            "Epoch 47: Cross-entropy: 21515.846\n",
            "Epoch 48: Cross-entropy: 20846.955\n",
            "Epoch 49: Cross-entropy: 20853.363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char-3.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1gVLieXS-gK",
        "outputId": "aab0912f-4800-4991-ce3e-f8cd7254cba9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file3 = \"/sherlock.txt\"\n",
        "raw_txt3 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt3 = raw_txt3.lower()\n",
        "raw_txt3 = raw_txt3[:50000]\n",
        "seq_len = 100\n",
        "start = np.random.randint(0, len(raw_txt3)-seq_len)\n",
        "prompt = raw_txt3[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "n-mGdxYNS-iF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1gh6-4AS-lI",
        "outputId": "284aa8e3-4381-4188-f776-20ef78111004"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " lucky appearance\n",
            "saved the bridegroom from having to sally out into the streets in\n",
            "search of a best\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            " man. bnd the has the fielt sespous of iis own high-power\n",
            "lenses, would bolngs has to be to mittle metters to me to be ro a creat belicate that i have made myself clear?\"\n",
            "\n",
            "\"i am to be neanly give myst my friend's amazing powers of importance to le to be ro rilence for the pest roint in the conningst of the most singular that the would has sriee and laughed again, in the count von kramm.\"\n",
            "\n",
            "\"then i should have thought a little more. j had not in the past which he had apparently has been myst be an alieied.\"\n",
            "\n",
            "\"to i have not seen, bnd i not be bought under hy wiich i evpected. it was a lews in the oart which has been wayled in the morning. and the world has seen, but as a\n",
            "lover he would have that he will be of the ouher, while a lews then her husband by the ttreet. \n",
            "\n",
            "\"mre iade!and a sueft little prince of a lettle maneau which had been lauely. yhich ie dould not the peculiar construction of the most serulde to the thought, when he stost her own healest and puesteon, but the could not ho fi\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this model the performance was a little better. The sentences are clearer and I can see that where the model got confused with some of the vowels. "
      ],
      "metadata": {
        "id": "u5GZ5ugXbZRl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0gAepYTbvf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}