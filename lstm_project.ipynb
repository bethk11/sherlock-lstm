{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBIhO/asQRJUj/0HbTUUW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pearpare/sherlock-lstm/blob/main/lstm_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elisabeth Kam (etk45) "
      ],
      "metadata": {
        "id": "k7omi9ubTpvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UMgIBMGO23qL"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "D-KWPTAI3LW_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/sherlock.txt\"\n",
        "raw_txt = open(filename, 'r', encoding = 'utf-8').read()\n",
        "#raw_txt = [raw_txt.replace(' ', '') for raw in raw_txt]"
      ],
      "metadata": {
        "id": "ONEbVIye3LZ1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_txt = raw_txt.lower()\n",
        "raw_txt = raw_txt[:50000]\n",
        "chars = sorted(list(set(raw_txt)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "_kG8Kvth3Lc1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_txt)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total characters: \", n_chars)\n",
        "print(\"Total vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pikuR7mS3Lgy",
        "outputId": "8d9d3cd6-b0bc-4402-a39f-2382b50808d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters:  50000\n",
            "Total vocab:  44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset of input to output pairs encoded as integers\n",
        "char_seq_len = 50\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, n_chars - char_seq_len, 1):\n",
        "    seq_in = raw_txt[i:i + char_seq_len]\n",
        "    seq_out = raw_txt[i + char_seq_len]\n",
        "    X_data.append([char_to_int[char] for char in seq_in])\n",
        "    y_data.append(char_to_int[seq_out])\n",
        "    \n",
        "n_patterns = len(X_data)\n",
        "print(\"Total patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60zDlAPn3LiZ",
        "outputId": "a2a5a7da-da04-40cd-fb3f-136b2e11df76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total patterns:  49950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X_data, dtype=torch.float32).reshape(n_patterns, char_seq_len, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(y_data)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XBlzNy33LlG",
        "outputId": "6778d65e-7aed-4eff-e7e4-aec243fd949a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([49950, 50, 1]) torch.Size([49950])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data "
      ],
      "metadata": {
        "id": "K0CL71z13Lm8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_zBvyQ3Lps",
        "outputId": "9780b45f-dd93-4027-c486-c367b5d7b9c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class bookModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout = 0.2)\n",
        "        self.dropout = nn.Dropout(0.2) #could try changing droput values for fun \n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x): \n",
        "        x, _ = self.lstm(x)\n",
        "        # takes only the last output \n",
        "        x = x[:, -1, :]\n",
        "        # produce output \n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x "
      ],
      "metadata": {
        "id": "U2wA96ko3pJP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "batch_size = 128 \n",
        "model = bookModel()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle = True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader: \n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #Validation Time\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.3f\" % (epoch, loss))\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87zR-ub73pMF",
        "outputId": "b7dcec0c-fe02-4417-d20d-42d36f9abb5b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 146138.828\n",
            "Epoch 1: Cross-entropy: 134399.094\n",
            "Epoch 2: Cross-entropy: 129455.500\n",
            "Epoch 3: Cross-entropy: 125558.992\n",
            "Epoch 4: Cross-entropy: 121896.844\n",
            "Epoch 5: Cross-entropy: 118509.797\n",
            "Epoch 6: Cross-entropy: 114714.719\n",
            "Epoch 7: Cross-entropy: 111921.219\n",
            "Epoch 8: Cross-entropy: 109119.609\n",
            "Epoch 9: Cross-entropy: 106658.750\n",
            "Epoch 10: Cross-entropy: 105133.664\n",
            "Epoch 11: Cross-entropy: 103828.242\n",
            "Epoch 12: Cross-entropy: 99264.562\n",
            "Epoch 13: Cross-entropy: 98614.281\n",
            "Epoch 14: Cross-entropy: 94654.641\n",
            "Epoch 15: Cross-entropy: 93194.883\n",
            "Epoch 16: Cross-entropy: 90537.344\n",
            "Epoch 17: Cross-entropy: 88246.297\n",
            "Epoch 18: Cross-entropy: 86732.422\n",
            "Epoch 19: Cross-entropy: 84632.992\n",
            "Epoch 20: Cross-entropy: 83034.867\n",
            "Epoch 21: Cross-entropy: 80697.750\n",
            "Epoch 22: Cross-entropy: 80874.211\n",
            "Epoch 23: Cross-entropy: 79120.297\n",
            "Epoch 24: Cross-entropy: 75374.023\n",
            "Epoch 25: Cross-entropy: 74269.664\n",
            "Epoch 26: Cross-entropy: 73040.000\n",
            "Epoch 27: Cross-entropy: 71859.820\n",
            "Epoch 28: Cross-entropy: 70303.945\n",
            "Epoch 29: Cross-entropy: 67853.281\n",
            "Epoch 30: Cross-entropy: 66157.391\n",
            "Epoch 31: Cross-entropy: 64433.344\n",
            "Epoch 32: Cross-entropy: 63559.723\n",
            "Epoch 33: Cross-entropy: 61628.465\n",
            "Epoch 34: Cross-entropy: 60431.184\n",
            "Epoch 35: Cross-entropy: 61577.727\n",
            "Epoch 36: Cross-entropy: 58639.430\n",
            "Epoch 37: Cross-entropy: 59532.406\n",
            "Epoch 38: Cross-entropy: 57131.355\n",
            "Epoch 39: Cross-entropy: 54062.449\n",
            "Epoch 40: Cross-entropy: 53237.898\n",
            "Epoch 41: Cross-entropy: 52148.285\n",
            "Epoch 42: Cross-entropy: 51364.723\n",
            "Epoch 43: Cross-entropy: 50370.273\n",
            "Epoch 44: Cross-entropy: 50021.250\n",
            "Epoch 45: Cross-entropy: 48369.215\n",
            "Epoch 46: Cross-entropy: 47440.105\n",
            "Epoch 47: Cross-entropy: 46471.895\n",
            "Epoch 48: Cross-entropy: 46365.691\n",
            "Epoch 49: Cross-entropy: 46718.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, char_to_int, torch.load(\"single-char.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7L3GOWZ3pO5",
        "outputId": "f62ce5fa-efca-4787-90a0-be66cee1bb0a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a prompt here \n",
        "file = \"/sherlock.txt\"\n",
        "raw_txt2 = open(file, 'r', encoding = 'utf-8').read()\n",
        "raw_txt2 = raw_txt2.lower()\n",
        "raw_txt = raw_txt[:50000]\n",
        "seq_len = 50\n",
        "start = np.random.randint(0, len(raw_txt2)-seq_len)\n",
        "prompt = raw_txt2[start:start+seq_len]\n",
        "pattern = [char_to_int[c] for c in prompt]"
      ],
      "metadata": {
        "id": "JU78bFfV3pRw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"Prompt ends here.\")\n",
        "print(\"\\n\")\n",
        "print(\"Result:\")\n",
        "with torch.no_grad():\n",
        "  for i in range(1000):\n",
        "    #format input array of int into pytorch tensor \n",
        "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    #genreate logits as output from the model \n",
        "    pred = model(x.to(device))\n",
        "    #convert logits into one character\n",
        "    index = int(pred.argmax())\n",
        "    result = int_to_char[index]\n",
        "    print(result, end=\"\")\n",
        "    #append the new character into the prompt for the next iteration\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfUvOe-p3pUP",
        "outputId": "02fae464-c314-4c37-d4e8-69e0677d4ef4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "\n",
            "written in a hurry and dipped her pen too deep. i\n",
            "Prompt ends here.\n",
            "\n",
            "\n",
            "Result:\n",
            "olmes which he had a pat which witl the rteption wiich i should rur at the eall. he was sapa in the coune to said it an insersence that the woole oas enueh that i was adrertarion to hore teat it on dirertalce.\"\n",
            "\n",
            "\"i whilk in the lady the house of the mat who was tuitted into the room which he had a pat woon the steption wiich i saw see hoom the room whth he was soon at ar elende of the simtleraph which he was satsing saper which had been aben of the stier, she was hou the soom with the shotograph in the matker was stickciny tign the paid in the steption wiich was a poall pooked to suos the soom which ie his welled upon the rtoeet. \n",
            "\"i co not with you will be good in the lat whin some cound have alenaele. it was a lanyer with the siotograph which he had a comatitned an immeriant wou know the room wiich i suselfeny of the soalet of the simtleraph which he has eeen and oattered to miss the stoeet. \n",
            "it was a goack and dall to be gone in the mat who was the mat who was tureet, and you will r\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}